{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb43105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import os\n",
    "import pandas as pd \n",
    "import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "#@title Install and Import Dependencies\n",
    "\n",
    "# this assumes that you have a relevant version of PyTorch installed\n",
    "# !pip install -q torchaudio\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "OUT_FILE = \"results.txt\"\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "# download example\n",
    "# torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7298df",
   "metadata": {},
   "source": [
    "## Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8883cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute model size  \n",
    "def compute_model_size(model):\n",
    "    num_params = np.sum([param.numel() for param in model.parameters()])\n",
    "    dtype = next(model.parameters()).dtype\n",
    "    num_bits = int(re.search(r\"\\d+$\", str(dtype)).group(0))\n",
    "    size_in_bytes = float(num_params * num_bits / (8 * 1e6))\n",
    "    return size_in_bytes, int(num_params)\n",
    "\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    def internal(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        res = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        lag = end - start\n",
    "        return res, lag\n",
    "    return internal    \n",
    "\n",
    "\n",
    "#compute metric for this sample\n",
    "\n",
    "def compute_metrics(y_true, y_pred, latency):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    prec, recall, f1 = precision_score(y_true, y_pred), recall_score(y_true, y_pred) , f1_score(y_true, y_pred)\n",
    "    res = {\"prec\": prec, \"recall\": recall, \"f1\": f1, \"latency\":latency }\n",
    "    return res\n",
    "\n",
    "def dump_results(file_path, res:list):\n",
    "    lines_to_dump = []\n",
    "    if not os.path.exists(file_path):\n",
    "        header = [\"time\", \"model\", \"prec\", \"recall\", \"f1\", \"test_dataset\", \"num_params\", \"size(MB)\", \"latency\"]\n",
    "        lines_to_dump.append(header)\n",
    "    lines_to_dump.append(res)\n",
    "    with open(file_path, \"a+\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for line in lines_to_dump:\n",
    "            writer.writerow(line)\n",
    "\n",
    "def process_rttm(file_path):\n",
    "    df = pd.read_csv(file_path, sep=\" \", names=[\"type\", \"file_id\", \"channel_id\", \"start\", \"duration\", \"orthography\", \"speaker_type\", \"speaker_name\", \"confidence_score\", \"other\"])\n",
    "    df['start'] = df['start'].astype(float)\n",
    "    df['duration'] = df['duration'].astype(float)\n",
    "    df['end'] = df['start']+ df['duration']\n",
    "    return df\n",
    "\n",
    "def extract_start_end(df):\n",
    "    return [{\"start\": float(df['start'][i]), \"end\": float(df['end'][i])} for i in range(len(df['start']))]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6940dd",
   "metadata": {},
   "source": [
    "# SILERO-V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085dcfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ********************************SILERO****************************************************8\n",
    "USE_PIP = True # download model using pip package or torch.hub\n",
    "USE_ONNX = False # change this to True if you want to test onnx model\n",
    "if USE_ONNX:\n",
    "    !pip install -q onnxruntime\n",
    "if USE_PIP:\n",
    "  !pip install -q silero-vad\n",
    "  from silero_vad import (load_silero_vad,\n",
    "                          read_audio,\n",
    "                          get_speech_timestamps,\n",
    "                          save_audio,\n",
    "                          VADIterator,\n",
    "                          collect_chunks)\n",
    "  model = load_silero_vad(onnx=USE_ONNX)\n",
    "else:\n",
    "  model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                                model='silero_vad',\n",
    "                                force_reload=True,\n",
    "                                onnx=USE_ONNX)\n",
    "\n",
    "  (get_speech_timestamps,\n",
    "  save_audio,\n",
    "  read_audio,\n",
    "  VADIterator,\n",
    "  collect_chunks) = utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b726b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.850376, 462594)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute model size \n",
    "size, num_params = compute_model_size(model)\n",
    "size, num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd982b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mesure inference speed  for 1 wav file ( ~ 10 s )\n",
    "wav_file_path = \"/home/yehoshua/projects/silero-vad/en_example.wav\"\n",
    "get_speech_timestamps = timeit(get_speech_timestamps)\n",
    "wav = read_audio(wav_file_path, sampling_rate=SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "725f794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_extract(wav_file_path, label_file_path):\n",
    "    # audio = Audio(wav_file_path)\n",
    "    wav = read_audio(wav_file_path, sampling_rate=SAMPLING_RATE)\n",
    "    # get speech timestamps from full audio file\n",
    "    speech_timestamps, latency = get_speech_timestamps(wav, model, sampling_rate=SAMPLING_RATE, return_seconds=True)\n",
    "    labels = open(label_file_path, \"r\").read().split(\",\")[1:]\n",
    "    labels = [{\"start\": float(s), \"end\":float(e), \"voice\": float(v)} for s, e, v  in [[labels[i], labels[i+1], labels[i+2]] for i in range(0,len(labels) -2, 3)]]\n",
    "    filtered_labels = [{k:float(v) for k, v in label.items() if label['voice'] == 1} for label in labels]\n",
    "    filtered_labels = [dic for dic in filtered_labels if dic != dict()]\n",
    "    return speech_timestamps,  filtered_labels, latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9900b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_mask(labels, preds, step=31.5e-3):\n",
    "    \"\"\"\n",
    "    Creates binary masks for labels and predictions based on time intervals\n",
    "    and a given step size, by checking each cursor point against all segments.\n",
    "\n",
    "    Args:\n",
    "        labels (list of dict): List of ground truth segments [{'start': x, 'end': y}].\n",
    "        preds (list of dict): List of predicted segments [{'start': x, 'end': y, 'voice': 1.0}].\n",
    "        step (float): The time step (e.g., 31.25e-3 for 31.25 ms).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists (labels_mask, preds_mask),\n",
    "               where each list contains 0s and 1s representing non-speech/speech\n",
    "               at each `step` interval.\n",
    "    \"\"\"\n",
    "    labels_mask = []\n",
    "    preds_mask = []\n",
    "\n",
    "    # Clean and filter predicted segments (only consider 'voice': 1.0)\n",
    "    # Ensure segments have 'start' and 'end' keys\n",
    "    valid_preds = [d for d in preds if isinstance(d, dict) and 'start' in d and 'end' in d and d.get('voice', 1.0) == 1.0]\n",
    "\n",
    "    # Filter labels to ensure they have 'start' and 'end' keys\n",
    "    valid_labels = [d for d in labels if isinstance(d, dict) and 'start' in d and 'end' in d]\n",
    "\n",
    "\n",
    "    # Determine the maximum end time across both labels and valid_preds\n",
    "    # This ensures the mask covers the full extent of relevant activity.\n",
    "    max_end_time = 0.0\n",
    "    if valid_labels:\n",
    "        max_end_time = max(max_end_time, max(d['end'] for d in valid_labels))\n",
    "    if valid_preds:\n",
    "        max_end_time = max(max_end_time, max(d['end'] for d in valid_preds))\n",
    "\n",
    "    # If there are no segments at all, return empty masks\n",
    "    if max_end_time == 0.0:\n",
    "        return [], []\n",
    "\n",
    "    # Iterate through the timeline at fixed steps\n",
    "    cursor = 0.0\n",
    "    \n",
    "    # CRITICAL CHANGE HERE: Use a while loop with a slight buffer\n",
    "    # The `+ step * 0.5` is a common trick to handle floating point inaccuracies\n",
    "    # and ensure that if max_end_time falls exactly on a step boundary,\n",
    "    # or just slightly past it due to internal calculation, the loop runs one more time\n",
    "    # to cover the interval up to max_end_time.\n",
    "    while cursor < max_end_time + step * 0.5: # Run slightly beyond max_end_time\n",
    "        # Check if the current cursor position falls within any ground truth segment\n",
    "        is_label_speech = 0\n",
    "        for segment in valid_labels:\n",
    "            # Check if cursor is within [start, end)\n",
    "            if segment['start'] <= cursor < segment['end']:\n",
    "                is_label_speech = 1\n",
    "                break # Found an overlapping label, no need to check further for this cursor\n",
    "        labels_mask.append(is_label_speech)\n",
    "\n",
    "        # Check if the current cursor position falls within any predicted segment\n",
    "        is_pred_speech = 0\n",
    "        for segment in valid_preds:\n",
    "            # Check if cursor is within [start, end)\n",
    "            if segment['start'] <= cursor < segment['end']:\n",
    "                is_pred_speech = 1\n",
    "                break # Found an overlapping prediction, no need to check further for this cursor\n",
    "        preds_mask.append(is_pred_speech)\n",
    "\n",
    "        cursor += step # Move to the next time step\n",
    "        \n",
    "    return labels_mask, preds_mask\n",
    "# labels_res, preds_res = create_mask(filtered_labels, speech_timestamps, 31.5e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a1469fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset example and compute timestamps with VAD\n",
    "test_dir = \"ten_vad/testset\"\n",
    "dic = {'wav': sorted([os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.endswith('.wav')], key=lambda f: int(re.search(r'(\\d+)', f).group(1)) if re.search(r'(\\d+)', f) else float('inf')), \n",
    "       \"time_stamp\": sorted([os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.endswith('.scv')], key=lambda f: int(re.search(r'(\\d+)', f).group(1)) if re.search(r'(\\d+)', f) else float('inf'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b5fcdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration on dataset: 30it [00:02, 10.54it/s]\n"
     ]
    }
   ],
   "source": [
    "def run_inference_silero(dic):\n",
    "    res = []\n",
    "    y_trues = []\n",
    "    for (wav_path, label_path) in tqdm(zip(dic[\"wav\"], dic['time_stamp']), desc=\"iteration on dataset\"):\n",
    "        timestamps, labels, latency = compute_and_extract(wav_path, label_path)\n",
    "        y_true, y_pred =  create_mask(labels, timestamps)\n",
    "        y_trues.append(y_true)\n",
    "        res_sample = compute_metrics(y_true, y_pred, latency)\n",
    "        res += [res_sample]\n",
    "    return res, y_trues\n",
    "        \n",
    "res, y_trues = run_inference_silero(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6a47bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#post process metrics \n",
    "prec = np.mean([dic['prec'] for dic in res])\n",
    "recall = np.mean([dic['recall'] for dic in res])\n",
    "f1 = np.mean([dic['f1'] for dic in res])\n",
    "latency = np.mean([dic['latency'] for dic in res]) \n",
    "dataset = \"ten_vad_testset\"\n",
    "model_name = \"silero-v5\"\n",
    "date = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9221faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_silero_summary = [date, model_name, prec, recall, f1, dataset, num_params, size, latency]\n",
    "dump_results(OUT_FILE, res_silero_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8dbe3f",
   "metadata": {},
   "source": [
    "## *TEN VAD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a36d97cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./ten_vad/testset/testset-audio-01pred.csv', './ten_vad/testset/testset-audio-02pred.csv', './ten_vad/testset/testset-audio-03pred.csv', './ten_vad/testset/testset-audio-04pred.csv', './ten_vad/testset/testset-audio-05pred.csv', './ten_vad/testset/testset-audio-06pred.csv', './ten_vad/testset/testset-audio-07pred.csv', './ten_vad/testset/testset-audio-08pred.csv', './ten_vad/testset/testset-audio-09pred.csv', './ten_vad/testset/testset-audio-10pred.csv', './ten_vad/testset/testset-audio-11pred.csv', './ten_vad/testset/testset-audio-12pred.csv', './ten_vad/testset/testset-audio-13pred.csv', './ten_vad/testset/testset-audio-14pred.csv', './ten_vad/testset/testset-audio-15pred.csv', './ten_vad/testset/testset-audio-16pred.csv', './ten_vad/testset/testset-audio-17pred.csv', './ten_vad/testset/testset-audio-18pred.csv', './ten_vad/testset/testset-audio-19pred.csv', './ten_vad/testset/testset-audio-20pred.csv', './ten_vad/testset/testset-audio-21pred.csv', './ten_vad/testset/testset-audio-22pred.csv', './ten_vad/testset/testset-audio-23pred.csv', './ten_vad/testset/testset-audio-24pred.csv', './ten_vad/testset/testset-audio-25pred.csv', './ten_vad/testset/testset-audio-26pred.csv', './ten_vad/testset/testset-audio-27pred.csv', './ten_vad/testset/testset-audio-28pred.csv', './ten_vad/testset/testset-audio-29pred.csv', './ten_vad/testset/testset-audio-30pred.csv']\n"
     ]
    }
   ],
   "source": [
    "#********************************TENVAD****************************************************8\n",
    "ten_vad_preds_file = sorted(glob.glob(\"./ten_vad/testset/*pred.csv\"), key=lambda x: int(re.search(\"\\d+\", x).group(0)))\n",
    "print(ten_vad_preds_file)\n",
    "ten_vad_pred = []\n",
    "ten_vad_latency = []\n",
    "for preds_file in ten_vad_preds_file:\n",
    "    df = pd.read_csv(preds_file)\n",
    "    preds = df.iloc[:, 1].tolist()\n",
    "    latency = np.sum(df.iloc[:, 2].tolist())\n",
    "    ten_vad_pred.append(preds)\n",
    "    ten_vad_latency.append(latency)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99463a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ten_vad = []\n",
    "for pred, label, latency in zip(ten_vad_pred, y_trues, ten_vad_latency):\n",
    "    len_pred, len_label = len(pred), len(label)\n",
    "    min_len = min(len_pred, len_label)\n",
    "    pred, label = pred[:min_len], label[:min_len]\n",
    "    res_ten_vad.append(compute_metrics(np.array(label), np.array(pred), latency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "808e8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = np.mean([dic['prec'] for dic in res_ten_vad])\n",
    "recall = np.mean([dic['recall'] for dic in res_ten_vad])\n",
    "f1 = np.mean([dic['f1'] for dic in res_ten_vad])\n",
    "latency = np.mean([dic['latency'] for dic in res_ten_vad])\n",
    "dataset = \"ten_vad_testset\"\n",
    "model_name = \"ten_vad\"\n",
    "num_params = \"NA\"\n",
    "size = \"0.35\"\n",
    "date = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "res_ten_vad_summary = [date, model_name, prec, recall, f1, dataset, num_params, size, latency]\n",
    "dump_results(OUT_FILE, res_ten_vad_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313f102",
   "metadata": {},
   "source": [
    "## PYANONOTE AUDIO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a8ecc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828254a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yehoshua/.pyenv/versions/3.11.9/lib/python3.11/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/059e96f964841d40f1a5e755bb7223f76666bba4/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.7.1, yours is 2.7.0+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "# 1. visit hf.co/pyannote/segmentation and accept user conditions\n",
    "# 2. visit hf.co/settings/tokens to create an access token\n",
    "# 3. instantiate pretrained voice activity detection pipeline\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\",\n",
    "                                    use_auth_token=os.environ['HF_TOKEN'])\n",
    "output = pipeline(\"en_example.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d785c9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.891512, 1472878)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyannote_vad_model = pipeline._segmentation.model\n",
    "size = compute_model_size(pyannote_vad_model)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b442a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_FILE = \"tmp.csv\"\n",
    "os.makedirs(\"pyannote_results\", exist_ok=True)\n",
    "pyannote_preds = []\n",
    "for wav_file in dic['wav']:\n",
    "    output = pipeline(wav_file)\n",
    "    with open(TMP_FILE, \"w\") as f:\n",
    "        rttm = output.write_rttm(f)\n",
    "    df = process_rttm(TMP_FILE)\n",
    "    time_stamps = extract_start_end(df)\n",
    "    _, labels, latency = compute_and_extract(wav_file, wav_file.replace(\".wav\", \".scv\"))\n",
    "    _, y_preds  = create_mask(labels, time_stamps)\n",
    "    pyannote_preds.append(y_preds)\n",
    "\n",
    "res_pyannote = []\n",
    "for pred, label in zip(pyannote_preds, y_trues):\n",
    "    len_pred, len_label = len(pred), len(label)\n",
    "    min_len = min(len_pred, len_label)\n",
    "    pred, label = pred[:min_len], label[:min_len]\n",
    "    res_pyannote.append(compute_metrics(np.array(label), np.array(pred), \"NA\"))\n",
    "    \n",
    "    \n",
    "prec = np.mean([dic['prec'] for dic in res_pyannote])\n",
    "recall = np.mean([dic['recall'] for dic in res_pyannote])\n",
    "f1 = np.mean([dic['f1'] for dic in res_ten_vad])\n",
    "latency = \"NA\"\n",
    "dataset = \"ten_vad_testset\"\n",
    "model_name = \"pyannote\"\n",
    "num_params = \"1472878\"\n",
    "size = \"5.891512\"\n",
    "date = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "pyannote_summary = [date, model_name, prec, recall, f1, dataset, num_params, size, latency]\n",
    "dump_results(OUT_FILE, pyannote_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be5a05b",
   "metadata": {},
   "source": [
    "## TR-VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0652019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "tr_vad_preds = []\n",
    "tr_vad_latencies = []\n",
    "with open(\"tr_vad_preds.csv\", \"r\") as f: \n",
    "    csv_reader = csv.reader(f)\n",
    "    for line in csv_reader:\n",
    "        tr_vad_preds.append([int(elem) for elem in line[2:]])\n",
    "        tr_vad_latencies.append(float(line[1]))\n",
    "\n",
    "\n",
    "\n",
    "res_tr_vad = []\n",
    "for pred, label, latency in zip(tr_vad_preds, y_trues, tr_vad_latencies):\n",
    "    len_pred, len_label = len(pred), len(label)\n",
    "    min_len = min(len_pred, len_label)\n",
    "    pred, label = pred[:min_len], label[:min_len]\n",
    "    res_tr_vad.append(compute_metrics(np.array(label), np.array(pred), latency))\n",
    "    \n",
    "    \n",
    "prec = np.mean([dic['prec'] for dic in res_tr_vad])\n",
    "recall = np.mean([dic['recall'] for dic in res_tr_vad])\n",
    "f1 = np.mean([dic['f1'] for dic in res_tr_vad])\n",
    "latency = np.mean([dic['latency'] for dic in res_tr_vad])\n",
    "dataset = \"ten_vad_testset\"\n",
    "model_name = \"tr_vad\"\n",
    "num_params = \"376000\"\n",
    "size = \"1.504\"\n",
    "date = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "tr_vad_summary = [date, model_name, prec, recall, f1, dataset, num_params, size, latency]\n",
    "dump_results(OUT_FILE, tr_vad_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c322b85",
   "metadata": {},
   "source": [
    "## SPEECHBRAIN: crdnn trained on libriparty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cae230a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.inference.VAD import VAD\n",
    "\n",
    "VAD = VAD.from_hparams(source=\"speechbrain/vad-crdnn-libriparty\", savedir=\"pretrained_models/vad-crdnn-libriparty\")\n",
    "boundaries = VAD.get_speech_segments(\"speechbrain/vad-crdnn-libriparty/example_vad.wav\")\n",
    "speechbrain_preds = []\n",
    "speechbrain_latencies = []\n",
    "for wav_file in dic['wav']:\n",
    "    start = time.time()\n",
    "    outputs = VAD.get_speech_segments(wav_file)\n",
    "    end = time.time()\n",
    "    latency = end - start\n",
    "    time_stamps = [{\"start\": t[0], \"end\":t[1]} for t in outputs]\n",
    "    _, labels, latency = compute_and_extract(wav_file, wav_file.replace(\".wav\", \".scv\"))\n",
    "    _, y_preds  = create_mask(labels, time_stamps)\n",
    "    speechbrain_preds.append(y_preds)\n",
    "    speechbrain_latencies.append(latency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07d661f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.438976, 109744)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speechbrain_vad = VAD\n",
    "compute_model_size(speechbrain_vad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe83e791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yehoshua/.pyenv/versions/3.11.9/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "res_speechbrain = []\n",
    "for pred, label, latency in zip(speechbrain_preds, y_trues, speechbrain_latencies):\n",
    "    len_pred, len_label = len(pred), len(label)\n",
    "    min_len = min(len_pred, len_label)\n",
    "    pred, label = pred[:min_len], label[:min_len]\n",
    "    res_speechbrain.append(compute_metrics(np.array(label), np.array(pred), latency))\n",
    "    \n",
    "    \n",
    "prec = np.mean([dic['prec'] for dic in res_speechbrain])\n",
    "recall = np.mean([dic['recall'] for dic in res_speechbrain])\n",
    "f1 = np.mean([dic['f1'] for dic in res_speechbrain])\n",
    "latency = np.mean([dic['latency'] for dic in res_speechbrain])\n",
    "dataset = \"ten_vad_testset\"\n",
    "model_name = \"speechbrain\"\n",
    "num_params = \"109744\"\n",
    "size = \"0.438976\"\n",
    "date = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "speechbrain_summary = [date, model_name, prec, recall, f1, dataset, num_params, size, latency]\n",
    "dump_results(OUT_FILE, speechbrain_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
